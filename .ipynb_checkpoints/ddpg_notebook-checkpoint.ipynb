{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise actor network\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self,state_size,action_size,seed,fc1_units=400,fc2_units=300):\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.input_actor = nn.Linear(state_size,fc1_units)\n",
    "        self.hidden_actor = nn.Linear(fc1_units,fc2_units)\n",
    "        self.output_actor = nn.Linear(fc2_units,action_size)\n",
    "        self.reset_parameters()\n",
    "    def reset_parameters(self):\n",
    "        self.input_actor.weight.data.uniform_(*hidden_init(self.input_actor))\n",
    "        self.hidden_actor.weight.data.uniform_(*hidden_init(self.hidden_actor))\n",
    "        self.output_actor.weight.data.uniform_(-3e-3, 3e-3)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.input_actor(x))\n",
    "        x = F.relu(self.hidden_actor(x))\n",
    "        x = F.tanh(self.output_actor(x))\n",
    "        return x\n",
    "        \n",
    "#initialise critic network\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size,action_size,seed,fc1_critic = 400, fc2_critic = 300):\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.input_critic = nn.Linear(state_size,fc1_critic)\n",
    "        self.hidden_critic = nn.Linear(fc1_critic+action_size,fc2_critic)\n",
    "        self.output_critic  = nn.Linear(fc2_critic,1)\n",
    "        self.reset_parameters()\n",
    "    def reset_parameters(self):\n",
    "        self.input_critic.weight.data.uniform_(*hidden_init(self.input_critic))\n",
    "        self.hidden_critic.weight.data.uniform_(*hidden_init(self.hidden_critic))\n",
    "        self.output_critic.weight.data.uniform_(-3e-3, 3e-3)\n",
    "    def forward(self, state, action):\n",
    "        xs = F.relu(self.input_critic(state))\n",
    "        x = torch.cat((xs,action),dim=1)\n",
    "        x = F.relu(self.hidden_critic(x))\n",
    "        return self.output_critic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\" )\n",
    "class ReplayBuffer:\n",
    "    def __init__(self,batch_size,buffer_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.experience = namedtuple(\"experience\",field_names= [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    def sample(self):\n",
    "        experience = random.sample(self.memory, self.batch_size)\n",
    "        state = torch.from_numpy(np.vstack([e.state for e in experience if e is not None])).float().to(device)\n",
    "        action = torch.from_numpy(np.vstack([e.action for e in experience if e is not None])).float().to(device)\n",
    "        reward = torch.from_numpy(np.vstack([e.reward for e in experience if e is not None ])).float().to(device)\n",
    "        next_state = torch.from_numpy(np.vstack([e.next_state for e in experience if e is not None])).float().to(device)\n",
    "        done = torch.from_numpy(np.vstack([e.done for e in experience if e is not None])).astype(np.uint8).float().to(device)\n",
    "        \n",
    "        return (state, action, reward, next_state, done)\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lr_actor = 1e-4\n",
    "lr_critic = 1e-3\n",
    "BUFFER_SIZE = 1e5\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "WEIGHT_DECAY = 0\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, state_size, action_size,random_seed):\n",
    "        super(Agent, self).__init__()\n",
    "        self.seed = random.seed(random_seed)\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.actor_local = Actor(state_size,action_size,random_seed).to(device)\n",
    "        \n",
    "        self.actor_local_optimizer = optim.Adam(self.actor_local.parameters(), lr=lr_actor)\n",
    "        \n",
    "        #actor target network initialize\n",
    "        self.actor_target = Actor(state_size,action_size,random_seed).to(device)\n",
    "        \n",
    "        #critic local network initialize\n",
    "        self.critic_local = Critic(state_size,action_size,random_seed).to(device)\n",
    "        self.critic_local_optimizer = optim.Adam(self.critic_local.parameters(), lr=lr_critic,weight_decay=WEIGHT_DECAY)\n",
    "        \n",
    "        #critic target network initialize\n",
    "        self.critic_target = Critic(state_size,action_size,random_seed).to(device)\n",
    "        self.noise = OUNoise(action_size, random_seed)\n",
    "        \n",
    "        \n",
    "        #initialize replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(BUFFER_SIZE,BATCH_SIZE)\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "    def step(self,state,action,reward,next_state,done):\n",
    "        self.replay_buffer.push(state,action,reward,next_state,done)\n",
    "        if len(self.replay_buffer) > BATCH_SIZE:\n",
    "            experiences = self.replay_buffer.sample()\n",
    "            self.learn(experiences,GAMMA)\n",
    "    def act(self, state, add_noise=True):\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().numpy()\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            action+=self.noise.sample()\n",
    "        return np.clip(action, -1, 1)\n",
    "    def learn(self,experiences,GAMMA):\n",
    "        state,action,reward,next_state,done = experiences\n",
    "        #action = self.actor_local(state)\n",
    "        \n",
    "        #predicted actions and q values from target net\n",
    "        action_next = self.actor_target(next_state)\n",
    "        Q_TARGET_NEXT = self.critic_target(next_state,action_next)\n",
    "        Q_TARGET = reward + (GAMMA*Q_TARGET_NEXT*(1-done))\n",
    "        \n",
    "        Q_VALUE =self.critic_local(state,action)\n",
    "        critic_loss = F.mse_loss(Q_VALUE,Q_TARGET)\n",
    "        #minimize the critic_loss\n",
    "        self.critic_local_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_local_optimizer.step()\n",
    "        \n",
    "        #calculate actor losses\n",
    "        actor_prediction = self.actor_local(state)\n",
    "        actor_loss = -self.critic_local(state,actor_prediction).mean()\n",
    "        #optimize actor loss\n",
    "        self.actor_local_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_local_optimizer.step()\n",
    "        print(actor_loss)\n",
    "        \n",
    "        self.smooth_update(self.critic_local, self.critic_target,TAU=1e-3)\n",
    "        self.smooth_update(self.actor_local, self.actor_target, TAU=1e-3)\n",
    "        \n",
    "        \n",
    "    def smooth_update(self, local_model, target_model, tau=1e-3):\n",
    "        \n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "        \n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n",
    "env = gym.make('Pendulum-v0')\n",
    "env.seed(2)\n",
    "ddpg_agent = Agent(state_size=3, action_size=1, random_seed=2)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-56-98b4792597c6>, line 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-56-98b4792597c6>\"\u001b[0;36m, line \u001b[0;32m27\u001b[0m\n\u001b[0;31m    fig = plt.figure()\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 1000\n",
    "max_timesteps = 100\n",
    "deque_len = 100\n",
    "print_every = 100\n",
    "scores = []\n",
    "scores_deque = deque(maxlen=print_every)\n",
    "for episode_index in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    ddpg_agent.reset()\n",
    "    \n",
    "    score = 0\n",
    "    for time_steps in range(max_timesteps):\n",
    "        action = ddpg_agent.act(state, add_noise=True)\n",
    "        print(action)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        ddpg_agent.step(state,action,reward,next_state,done)\n",
    "        state = next_state\n",
    "        score+=reward\n",
    "        if done:\n",
    "            break\n",
    "    scores.append(score)\n",
    "    scores_deque.append(score)\n",
    "    #print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode_index, np.mean(scores_deque)), end=\"\")\n",
    "\n",
    "    if episode_index%print_every == 0:\n",
    "        print('Episode Number : {} Average Score : {:.2f}'.format(episode_index, np.mean(scores_deque)))\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
